{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "######################################## PRE-PROCESSING ################################################\n",
    "### Scriviamo del codice che vada a concatenare i diversi CSV scaricati da PUBMED che fanno riferimento al medesimo anno.\n",
    "## Andiamo inoltre ad implementare del codice che vada a leggere i file txt scaricati da PUBMED contenenti l'Abstract in\n",
    "## modo da creare un secondo DataFrame con \"PMID\" e \"Abstract\".\n",
    "## Successivamente uniremo i due Dataframe per crearne uno solo sulla base del PMID comune, ottenendo cosi il nostro Dataset\n",
    "## completo di tutti gli articoli per tutti gli anni e avente come variabili :\n",
    "##  *** scrivere le variabili *****\n",
    "## Andiamo inoltre ad eliminare tutti i valori anomali quali:\n",
    "- Valori duplicati\n",
    "- Valori nulli\n",
    "# Eliminiamo le variabili che non sono d'interesse all'analisi da effettuare.\n",
    "# Procediamo con un ulteriore pulizia della colonna Authors, dove vengono ricercati tutti i valori sospetti che potrebbero\n",
    "# generare doppioni di autori e inficiare la qualitÃ  dell'analisi\n",
    "# Infine i nomi degli autori vengono standardizzati in lettere minuscole al fine di evitare la generazione di doppioni."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64050ed8",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f95b25",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Import Packages:\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e7e96",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "################################  CREAZIONE DEL PRIMO DATAFRAME #############################################\n",
    "### Caricamento dei vari Dataset:\n",
    "###### ANNO 1949 - 1999 #####\n",
    "t1_49_99 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/csv-coronaviru-set_1949_1999.csv\")\n",
    "###### ANNI 2000 - 2015 #####\n",
    "t1_00_15 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/csv-coronaviru-set_2000_2015.csv\")\n",
    "###### ANNO 2016 #####\n",
    "t1_16 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/csv-coronaviru-set_2016.csv\")\n",
    "###### ANNO 2017 #####\n",
    "t1_17 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/csv-coronaviru-set_2017.csv\")\n",
    "###### ANNO 2018 #####\n",
    "t1_18 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/csv-coronaviru-set_2018.csv\")\n",
    "###### ANNO 2019 #####\n",
    "t1_19 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/csv-coronaviru-set_2019.csv\")\n",
    "###### ANNO 2020 #####\n",
    "t1_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_1_1_20_._30_4_20(pt.1).csv\")\n",
    "t2_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_1_1_20_._30_4_20(pt.2).csv\")\n",
    "t3_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_1_5_20_._31_5_20.csv  \")\n",
    "t4_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_1_6_20_._15_7_20(pt.1).csv \")\n",
    "t5_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_1_6_20_._15_7_20(pt.2).csv  \")\n",
    "t6_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_16_7_20_._1_9_20(pt.1).csv \")\n",
    "t7_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_16_7_20_._1_9_20(pt.2).csv \")\n",
    "t8_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_02_9_20_._31_10_20(pt.1).csv\")\n",
    "t9_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_02_9_20_._31_10_20(pt.2).csv\")\n",
    "t10_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_1_11_20_._31_12_20(pt.1).csv\")\n",
    "t11_20 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2020/csv-coronaviru-set_1_11_20_._31_12_20(pt.2).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5a417",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###### ANNO 2021 #####\n",
    "t1_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_01_01_21_._25_01_21(pt.1).csv\")\n",
    "t2_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_01_01_21_._25_01_21(pt.2).csv\")\n",
    "t3_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_26_1_21_._31_3_21(pt.1).csv\")\n",
    "t4_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_26_1_21_._31_3_21(pt.2).csv \")\n",
    "t5_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_01_4_21_._31_5_21(pt.1).csv  \")\n",
    "t6_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_01_4_21_._31_5_21(pt.2).csv \")\n",
    "t7_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_01_6_21_._31_7_21(pt.1).csv \")\n",
    "t8_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_01_6_21_._31_7_21(pt.2).csv\")\n",
    "t9_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_1_8_21_._30_9_21(pt.1).csv\")\n",
    "t10_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_1_8_21_._30_9_21(pt.2).csv\")\n",
    "t11_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_01_10_21_._30_11_21(pt.1).csv\")\n",
    "t12_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_01_10_21_._30_11_21(pt.2).csv\")\n",
    "t13_21 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2021/csv-coronaviru-set_1_12_21_._31_12_21.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b346d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###### ANNO 2022 #####\n",
    "t1_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_01_01_2022(pt.1).csv\")\n",
    "t2_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_01_01_2022(pt.2).csv\")\n",
    "t3_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_03_01_2022_._31_03_2022(pt.1).csv\")\n",
    "t4_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_03_01_2022_._31_03_2022(pt.2).csv \")\n",
    "t5_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_01_04_22_._30_06_22(pt.1).csv  \")\n",
    "t6_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_01_04_22_._30_06_22(pt.2).csv \")\n",
    "t7_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_01_07_22_._30_09_22(pt.1).csv \")\n",
    "t8_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_01_07_22_._30_09_22(pt.2).csv\")\n",
    "t9_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_01_10_22_._31_12_22(pt.1).csv\")\n",
    "t10_22 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/2022/csv-coronaviru-set_01_10_22_._31_12_22(pt.2).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27712db",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###### ANNO 2023 #####\n",
    "t1_23 = pd.read_csv(\"./Dataset_CoronaVirus_1949_2023/csv-coronaviru-set_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5c6ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.concat([t1_49_99,t1_00_15,t1_16,t1_17,t1_18,t1_19,\n",
    "                            t1_20,t2_20,t3_20,t4_20,t5_20,t6_20, t7_20, t8_20, t9_20, t10_20, t11_20,\n",
    "                            t1_21,t2_21,t3_21,t4_21,t5_21,t6_21,t7_21,t8_21,t9_21,t10_21,t11_21,t12_21,t13_21,\n",
    "                            t1_22,t2_22,t3_22,t4_22,t5_22,t6_22,t7_22,t8_22,t9_22,t10_22,\n",
    "                            t1_23\n",
    "                            ], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c177c46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#INFO SUL PRIMO DATAFRAME :\n",
    "# df1_summary= df1.info()\n",
    "# print(df1_summary)\n",
    "### Verifichiamo la presenza di valori duplicati:\n",
    "# print(df1.duplicated())\n",
    "# Remove duplicates:\n",
    "df1 = df1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cb48b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f759da22",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##############################  CREAZIONE DEL SECONDO DATAFRAME #############################################\n",
    "#Il codice definisce una lista di campi che si vogliono estrarre dal file di testo, legge il file di testo\n",
    "come una lista di stringhe e divide le stringhe in righe separate. Poi per ogni riga, estrae il valore del campo\n",
    "corrispondente e lo aggiunge a un dizionario che rappresenta una riga del DataFrame.\n",
    "Infine, crea un DataFrame con tutte le righe e colonne corrispondenti ai campi definiti inizialmente e lo stampa a schermo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a6bed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004a16d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# definire la lista di campi da estrarre dal file di testo\n",
    "fields = ['PMID', 'AB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b464cd8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creare una lista vuota per memorizzare tutti i DataFrame\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ad803",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loop attraverso tutti i file txt nella cartella \"File_txt_for_Abstract\"\n",
    "for filename in os.listdir('./File_txt_for_Abstract/'):\n",
    "    if filename.endswith('.txt'):\n",
    "        # leggere il file di testo come una lista di stringhe\n",
    "        with open('./File_txt_for_Abstract/' + filename, 'r', encoding='utf-8') as f:\n",
    "            txt = f.read()\n",
    "\n",
    "        # split del testo in base ai caratteri \"PMID-\" e \"OWN -\"\n",
    "        txt_list = txt.split(\"PMID-\")[1:]\n",
    "\n",
    "        # inizializzazione delle liste che conterranno i dati estratti\n",
    "        pmid_list = []\n",
    "        abstract_list = []\n",
    "\n",
    "        # loop su tutte le sezioni del testo estratte\n",
    "        for t in txt_list:\n",
    "            # estrazione del PMID\n",
    "            pmid = t.split(\"\\n\")[0].strip()\n",
    "            pmid_list.append(pmid)\n",
    "\n",
    "            # estrazione dell'abstract\n",
    "            abstract_start = \"AB  - \"\n",
    "            abstract_end1 = \"FAU - \"\n",
    "            abstract_end2 = \"CI  - \"\n",
    "\n",
    "            if abstract_start in t:\n",
    "                if abstract_end1 in t:\n",
    "                    abstract = t.split(abstract_start)[1].split(abstract_end1)[0].strip()\n",
    "                elif abstract_end2 in t:\n",
    "                    abstract = t.split(abstract_start)[1].split(abstract_end2)[0].strip()\n",
    "                else:\n",
    "                    abstract = t.split(abstract_start)[1].strip()\n",
    "\n",
    "                # Rimuovi gli spazi dall'abstract\n",
    "                abstract = abstract.replace('\\n', '')\n",
    "\n",
    "            else:\n",
    "                abstract = \"\"\n",
    "\n",
    "            abstract_list.append(abstract)\n",
    "\n",
    "        # creazione del DataFrame\n",
    "        df = pd.DataFrame({\"PMID\": pmid_list, \"Abstract\": abstract_list})\n",
    "\n",
    "        # Aggiungi il dataframe alla lista di dataframe\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90cc3d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# concatenare tutti i DataFrame nella lista in un unico DataFrame\n",
    "df2 = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6b90e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "stampare il DataFrame\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0bc0ae",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#INFO SUL SECONDO DATAFRAME :\n",
    "# df2_summary= df2.info()\n",
    "# print(df2_summary)\n",
    "### Verifichiamo la presenza di valori duplicati:\n",
    "# print(df2.duplicated())\n",
    "# Remove duplicates:\n",
    "df2 = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fcf288",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### ^^^^^^^^^^^^^^^^^^^^^^     UNIONE DEI DUE DATAFRAME IN UNO SOLO:  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#converti il tipo di dati della colonna \"PMID\" in df1 in object\n",
    "df2['PMID'] = df2['PMID'].astype('int64')\n",
    "#unisce i due DataFrame\n",
    "merged_df = pd.merge(df1, df2, on='PMID')\n",
    "#visualizza il risultato\n",
    "# print(merged_df)\n",
    "df = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b1f54",
   "metadata": {
    "lines_to_next_cell": 1,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### ^^^^^^^^^^^ Verifichiamo la presenza di valori anomali quali NA e NaN\n",
    "print(df.isna().sum())\n",
    "### ^^^^^^^^^^^^ ELIMINIAMO QUELLE VARIABILI CHE NON RISULTANO UTILI AI FINI DELL'ANALISI E PRESENTANO VALORI ANOMALI (NaN)\n",
    "df.drop([\"PMCID\"],axis=1, inplace=True)\n",
    "df.drop([\"DOI\"],axis=1, inplace=True)\n",
    "df.drop([\"NIHMS ID\"],axis=1, inplace=True)\n",
    "df.drop([\"Create Date\"],axis=1, inplace=True)\n",
    "df.drop([\"Citation\"],axis=1, inplace=True)\n",
    "# print(df)\n",
    "#\n",
    "### ^^^^^^^^^^^^^^^^^^^ Ci assicuriamo di eliminare le righe che presentano eventuali valori nulli restanti\n",
    "print(df.isna().sum())\n",
    "df = df.dropna()\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1660c",
   "metadata": {
    "lines_to_next_cell": 1,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### ^^^^  Puliamo la colonna \"Authors\" da simboli non voluti e renderla minuscola ^^^^^\n",
    "### ^^^^^^^ (vengono eliminati anche i nomi delle organizzazioni dopo \";\" erano pochi)  ^^^^^^\n",
    "def clean_authors(authors):\n",
    "    # rimuove tutti i caratteri diversi da lettere, virgola, punto e virgola, parentesi tonde, spazi e trattino\n",
    "    cleaned_authors = re.sub(r'[^a-zA-Z,;\\(\\)\\-\\s]', '', authors)\n",
    "    # dividi la stringa di autori in una lista di autori separati dalla virgola\n",
    "    authors_list = cleaned_authors.split(\",\")\n",
    "    # per ogni autore, elimina tutti i caratteri dopo il punto e virgola nella stringa\n",
    "    authors_list = [author.split(\";\")[0] for author in authors_list]\n",
    "    # unisci la lista di autori in una stringa separata da virgole\n",
    "    cleaned_authors = \",\".join(authors_list)\n",
    "    # trasforma la stringa in minuscolo\n",
    "    cleaned_authors = cleaned_authors.lower()\n",
    "    # rimuove gli spazi all'inizio e alla fine della stringa\n",
    "    cleaned_authors = cleaned_authors.strip()\n",
    "    return cleaned_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d16298",
   "metadata": {
    "lines_to_next_cell": 2,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# applica la funzione alla colonna Authors del DataFrame\n",
    "df['Authors'] = df['Authors'].apply(clean_authors)\n",
    "# Converte i nomi degli autori principali in minuscolo\n",
    "df[\"First Author\"] = df[\"First Author\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb32507",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Verifichiamo la presenza di valori duplicati attraverso il PMID:\n",
    "# Rimozione delle righe duplicate, mantenendo solo quella con il valore meno recente di \"Publication Year\"\n",
    "df = df.sort_values(by=['PMID', 'Publication Year'], ascending=False).drop_duplicates(subset=['PMID'], keep='last')\n",
    "# Stampa del DataFrame dopo la rimozione delle righe duplicate\n",
    "# print(\"DataFrame dopo la rimozione delle righe duplicate:\")\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db8cf4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0b411",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ################################# SELEZIONE E EXPORT #########################################\n",
    "# # #Ci assicuriamo di analizzare gli articoli effettivamente pubblicati nell'anno desiderato:\n",
    "# df_selected = df.query('`Publication Year` >= 2000 and `Publication Year` <= 2019')\n",
    "df_selected = df.query('`Publication Year` == 2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a708c31",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # # Esporta il dataframe in un file CSV\n",
    "# ####df_selected.to_csv('Dataset_Corona_2023_COMPLETO===.csv', index=False)\n",
    "#esporta il DataFrame in un file Excel\n",
    "df_selected.to_excel('Dataset_Corona_2023_FINALE.xlsx', index=False, sheet_name='Sheet1', startrow=0, startcol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90deb645",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# apre il file Excel appena creato\n",
    "os.startfile('Dataset_Corona_2023_FINALE.xlsx')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}