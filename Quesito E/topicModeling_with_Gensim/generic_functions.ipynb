{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc033d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695a928",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "from pyLDAvis.gensim_models import prepare\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, output_file, save\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "from wordcloud import WordCloud\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(\n",
    "    ['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get',\n",
    "     'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack',\n",
    "     'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "        yield (sent)\n",
    "def process_words(texts,\n",
    "                  stop_words=stop_words,\n",
    "                  allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'],\n",
    "                  bigram_mod=None,\n",
    "                  trigram_mod=None):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.max_length = 10000000\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
    "    return texts_out\n",
    "def format_topics_sentences(model=None, corpus=None, texts=None):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(model[corpus]):\n",
    "        if len(row_list) == 0:\n",
    "            continue\n",
    "        row = row_list[0] if model.per_word_topics else row_list\n",
    "        if isinstance(row, tuple):\n",
    "            row = [row]\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = model.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(\n",
    "                    pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return (sent_topics_df)\n",
    "def visualize_topics(model, corpus):\n",
    "    vis = prepare(model, corpus, dictionary=model.id2word, mds='mmds')\n",
    "    pyLDAvis.save_html(vis, './report/topic_modeling_visualization.html')\n",
    "def show_topic_clusters(model, corpus, n_topics=10):\n",
    "    topic_weights = []\n",
    "    for i, row_list in enumerate(model[corpus]):\n",
    "        topic_weights.append([w for i, w in row_list[0]])\n",
    "    # Array of topic weights\n",
    "    arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "    # Keep the well separated points (optional)\n",
    "    arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "    # Dominant topic number in each doc\n",
    "    topic_num = np.argmax(arr, axis=1)\n",
    "    # tSNE Dimension Reduction\n",
    "    # t-distributed Stochastic Neighbor Embedding\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "    tsne_lda = tsne_model.fit_transform(arr)\n",
    "    # Plot the Topic Clusters using Bokeh\n",
    "    output_notebook()\n",
    "    file_name = 'report/topic_modeling_clusters.html'\n",
    "    output_file(file_name)\n",
    "    mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "    plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics),\n",
    "                  plot_width=900, plot_height=700)\n",
    "    plot.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1], color=mycolors[topic_num])\n",
    "    save(plot)\n",
    "# def show_word_cloud(model):\n",
    "#     for t in range(model.num_topics):\n",
    "#         plt.figure()\n",
    "#         plt.imshow(WordCloud().fit_words(dict(model.show_topic(t, 200))))\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()\n",
    "##===### Visualize WordCloud for each Topics in LDA Model:\n",
    "def show_word_cloud(lda_model, n_topics=6, n_words=20):\n",
    "    topics = lda_model.show_topics(num_topics=n_topics, num_words=n_words, formatted=False)\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    for i, ax in enumerate(axs.ravel()):\n",
    "        if i < n_topics:\n",
    "            words = dict(topics[i][1])\n",
    "            wc = WordCloud(background_color='black', max_words=200, width=800, height=400)\n",
    "            wc.generate_from_frequencies(words)\n",
    "            ax.imshow(wc, interpolation='bilinear')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"WordCloud Topic#{i}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee0c52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def show_results_coherence_plot(model, coherence_values, start, limit, step):\n",
    "    x = range(start, limit, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.title(model)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7a0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_evolution(lda_model, corpus, dataset):\n",
    "    # Get yearly topic distributions\n",
    "    yearly_topic_distributions = {}\n",
    "    for i, doc in enumerate(corpus):\n",
    "        year = dataset.iloc[i][\"Publication Year\"]\n",
    "        if year not in yearly_topic_distributions:\n",
    "            yearly_topic_distributions[year] = [0] * lda_model.num_topics\n",
    "        topics = lda_model.get_document_topics(doc)\n",
    "        for topic in topics:\n",
    "            topic_id = topic[0]\n",
    "            topic_weight = topic[1]\n",
    "            yearly_topic_distributions[year][topic_id] += topic_weight\n",
    "\n",
    "    # Plot topic evolution over years\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(lda_model.num_topics):\n",
    "        topic_id = i\n",
    "        topic_evolution = []\n",
    "        for year in sorted(yearly_topic_distributions.keys()):\n",
    "            topic_evolution.append(yearly_topic_distributions[year][topic_id])\n",
    "        ax.plot(sorted(yearly_topic_distributions.keys()), topic_evolution, label=f\"Topic {topic_id}\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Topic Frequency')\n",
    "    ax.set_title('Evolution of Topics')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
